<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pcgsepy.mapelites.emitters API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pcgsepy.mapelites.emitters</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import ABC, abstractmethod
from typing import Any, Dict, List, Tuple, Union
import logging

import numpy as np
import numpy.typing as npt
from sklearn.neural_network import MLPRegressor
from pcgsepy.config import BETA_A, BETA_B, CONTEXT_IDXS, CS_MAX_AGE, N_EPOCHS, USE_TORCH
from pcgsepy.mapelites.bin import MAPBin
from pcgsepy.mapelites.buffer import Buffer, mean_merge
from scipy.special import softmax
from sklearn.linear_model import LinearRegression
from sklearn.utils._testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.kernel_ridge import KernelRidge
from sklearn.neighbors import KNeighborsRegressor

logging.getLogger(&#39;mapelites&#39;).info(msg=f&#39;PyTorch set to {USE_TORCH}&#39;)

if USE_TORCH:
    from pcgsepy.nn.estimators import NonLinearEstimator, train_estimator
else:
    class NonLinearEstimator():
        def __init__(self,
                     xshape,
                     yshape):
            raise NotImplementedError(&#39;This object should never be instantiated&#39;)

        def train_estimator(estimator, xs, ys, n_epochs):
            raise NotImplementedError(&#39;This function should never be called&#39;)


class Emitter(ABC):
    def __init__(self) -&gt; None:
        super().__init__()
        self.name = &#39;abstract-emitter&#39;
        self.requires_init = False
        self.requires_pre = False
        self.requires_post = False
        self.diversity_weight = 0.
    
    @abstractmethod
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        raise NotImplementedError(f&#39;The {self.name} must override the `pick_bin` method!&#39;)
    
    def init_emitter(self,
                     **kwargs) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `init_emitter` method!&#39;)
    
    def pre_step(self,
                 **kwargs) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `pre_step` method!&#39;)
    
    def post_step(self,
                 **kwargs):
        raise NotImplementedError(f&#39;The {self.name} must override the `post_step` method!&#39;)
    
    @abstractmethod
    def reset(self) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `reset` method!&#39;)


class RandomEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create a random emitter class.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;random-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        &#34;&#34;&#34;Randomly return a bin among possible valid bins.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The randomly picked bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        fcs, ics = 0, 0
        selected = []
        while fcs &lt; 2 or ics &lt; 2:
            selected.append(bins.pop(np.random.choice(np.arange(len(bins)))))
            fcs += len(selected[-1]._feasible)
            ics += len(selected[-1]._infeasible)
        return selected
    
    def reset(self) -&gt; None:
        pass

    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;RandomEmitter&#39;:
        re = RandomEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re


class OptimisingEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create an optimising emitter.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;optimising-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The selected bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        sorted_bins = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
        fcs, ics = 0, 0
        selected = []
        while fcs &lt; 2 or ics &lt; 2:
            selected.append(sorted_bins.pop(0))
            fcs += len(selected[-1]._feasible)
            ics += len(selected[-1]._infeasible)
        return selected

    def reset(self) -&gt; None:
        pass
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitter&#39;:
        re = OptimisingEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re


class OptimisingEmitterV2(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create an optimising emitter (population-based).&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;optimising-emitter-v2&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[List[MAPBin]]:
        &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The selected bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        sorted_bins_f = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
        sorted_bins_i = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;infeasible&#39;), reverse=True)
        fcs, ics = 0, 0
        selected = [[], []]
        while fcs &lt; 2:
            selected[0].append(sorted_bins_f.pop(0))
            fcs += len(selected[0][-1]._feasible)
        while ics &lt; 2:
            selected[1].append(sorted_bins_i.pop(0))
            ics += len(selected[1][-1]._infeasible)
        return selected

    def reset(self) -&gt; None:
        pass
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitterV2&#39;:
        re = OptimisingEmitterV2()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re


class GreedyEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create a greedy emitter.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;greedy-emitter&#39;
        self.requires_pre = True
        self._last_selected: List[List[int]] = []
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        selected = [bins[idx] for idx in self._last_selected if bins[idx].non_empty(pop=&#39;feasible&#39;) or bins[idx].non_empty(pop=&#39;infeasbile&#39;)]
        return selected
    
    def reset(self) -&gt; None:
        self._last_selected = []

    def pre_step(self, **kwargs) -&gt; None:
        self._last_selected = []
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        for idx in idxs:
            self._last_selected.append(idx)
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight,
            &#39;last_selected&#39;: self._last_selected
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;GreedyEmitter&#39;:
        re = GreedyEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        re._last_selected = my_args[&#39;last_selected&#39;]
        return re


# --- Preference-Learning Emitters implementations below ---
#     These are specific implementations of the general framework.


class HumanPrefMatrixEmitter(Emitter):
    def __init__(self,
                 delta: float = 1.,
                 decay: float = 5e-2,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1) -&gt; None:
        &#34;&#34;&#34;Create a human preference-matrix emitter.

        Args:
            delta (float, optional): The preference increment. Defaults to `1`.
            decay (float, optional): The preference decay. Defaults to `5e-2`.
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to `gibbs`.
            epsilon (float, optional): The probability threshold used if sampling strategy is `epsilon-greedy`. Defaults to `9e-1`.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to `1e-1`.
        &#34;&#34;&#34;
        super().__init__()
        self.name = &#39;human-preference-matrix-emitter&#39;
        self.requires_init = True
        self.requires_post = True
        self.requires_pre = True
        
        self._delta = delta
        self._decay = decay
        self._prefs = None
        self._tot_actions = 0
        self._last_selected = []
        
        self.sampling_strategy = sampling_strategy  # epsilon_greedy or gibbs
        self.tau = tau
        self.epsilon = epsilon
        self._initial_tau = tau
        self._initial_epsilon = epsilon
        self.sampling_decay = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    def _build_pref_matrix(self,
                           bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; None:
        &#34;&#34;&#34;Build the preference matrix.

        Args:
            bins (np.ndarray[MAPBin]): The MAP-Elites bins.
        &#34;&#34;&#34;
        self._prefs = np.zeros(shape=bins.shape, dtype=np.float16)
        for (i, j), b in np.ndenumerate(bins):
            self._prefs[i, j] = self._delta if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasbile&#39;) else 0.
        
    def _get_n_new_bins(self,
                        bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; int:
        n_new_bins = 0
        for (_, _), b in np.ndenumerate(bins):
            css = [*b._feasible, *b._infeasible]
            for cs in css:
                if cs.age == CS_MAX_AGE:
                    n_new_bins += 1
                    break
        return n_new_bins
    
    def _reshape_matrix(self,
                        arr: np.typing.NDArray,
                        idx: Tuple[int, int]) -&gt; np.typing.NDArray:
        i, j = idx
        # create new matrix by coping preferences over to new column/rows
        # rows repetitions
        a = np.ones(shape=arr.shape[0], dtype=int)
        a[i] += 1
        # copy row
        arr = np.repeat(arr, repeats=a, axis=0)
        # columns repetitions
        a = np.ones(shape=arr.shape[1], dtype=int)
        a[j] += 1
        # copy column
        arr = np.repeat(arr, repeats=a, axis=1)
        return arr
    
    def _increase_preferences_res(self,
                                  idx: Tuple[int, int]) -&gt; None:
        assert self._prefs is not None, &#39;Human-preference emitter has not been initialized! Preference matrix has not been set.&#39;
        self._prefs = self._reshape_matrix(arr=self._prefs,
                                           idx=idx)
    
    def _decay_preferences(self) -&gt; None:
        self._prefs -= self._decay
        self._prefs[np.where(self._prefs &lt; 0)] = 0
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._prefs is not None, &#39;Human-preference emitter has not been initialized! Preference matrix has not been set.&#39;
        self._last_selected = []
        selected_bins = []
        non_empty = set([(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)])
        valid_prefs = set([tuple(x) for x in np.argwhere(self._prefs &gt; 0).tolist()])
        valid_idxs = list(non_empty.intersection(valid_prefs))
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: self._prefs[x], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            idxs = tuple(np.asarray(valid_idxs).transpose())
            logits = softmax(self._prefs[idxs] / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            valid_bins = bins[idxs]
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            self._last_selected.append(np.argwhere(bins == b).tolist()[0])
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins
    
    def init_emitter(self,
                     **kwargs) -&gt; None:
        assert self._prefs is None, f&#39;{self.name} has already been initialized!&#39;
        self._build_pref_matrix(bins=kwargs[&#39;bins&#39;])
        
    def pre_step(self, **kwargs) -&gt; None:
        assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
        bins: np.ndarray[MAPBin] = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        # get number of new/updated bins
        n_new_bins = self._get_n_new_bins(bins=bins)
        # update preference for selected bins
        for (i, j) in idxs:
            self._prefs[i, j] += 1.
            # if selected bin was just created, update parent bin accordingly
            if self._last_selected is not None:
                b = bins[i, j]
                css = [*b._feasible, *b._infeasible]
                for cs in css:
                    if cs.age == CS_MAX_AGE:
                        # we don&#39;t know which bin generated which, so update them all proportionally
                        for mn in self._last_selected:
                            self._prefs[mn[0], mn[1]] += 1 / n_new_bins
                        break
    
    def post_step(self,
                  bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; None:
        assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
        self._decay_preferences()

    def reset(self) -&gt; None:
        self._prefs = None
        self._tot_actions = 0
        self._tau = self._initial_tau
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;tot_actions&#39;: self._tot_actions,
            &#39;decay&#39;: self._decay,
            &#39;last_selected&#39;: self._last_selected,  # may need conversion tolist()
            &#39;prefs&#39;: self._prefs.tolist(),
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;sampling_strategy&#39;: self.sampling_strategy
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;HumanPrefMatrixEmitter&#39;:
        re = HumanPrefMatrixEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        re._tot_actions = my_args[&#39;tot_actions&#39;]
        re._decay = my_args[&#39;decay&#39;]
        re._last_selected = my_args[&#39;last_selected&#39;]  # may need conversion np.asarray
        re._prefs = np.asarray(my_args[&#39;prefs&#39;])
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_strategy = my_args[&#39;sampling_strategy&#39;]
        return re


class ContextualBanditEmitter(Emitter):
    def __init__(self,
                 n_features_context: int = len(CONTEXT_IDXS),
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a contextual bandit emitter.

        Args:
            n_features_context (int, optional): The number of features in the solution context. Defaults to len(CONTEXT_IDXS).
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 9e-1.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to 1e-1.
            estimator (str, optional): The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;contextual-bandit-emitter&#39;
        self.requires_pre: bool = True
        
        self._n_features_context: int = n_features_context
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: Union[LinearRegression, NonLinearEstimator, MLPRegressor] = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self._initial_epsilon: float = epsilon
        self.epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        if self._estimator == &#39;linear&#39;:
            self.estimator = LinearRegression().fit(X=xs, y=ys)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        elif self._estimator == &#39;mlp&#39;:
            # if USE_TORCH:
            #     self.estimator = NonLinearEstimator(xshape=self._n_features_context,
            #                                         yshape=1)
            #     train_estimator(estimator=self._estimator,
            #                     xs=xs,
            #                     ys=ys,
            #                     n_epochs=20)
            # else:
                self.estimator = MLPRegressor(hidden_layer_sizes=(100, 100),
                                              activation=&#39;relu&#39;,
                                              alpha=1e-4,
                                              solver=&#39;lbfgs&#39;,
                                              verbose=1 if logging.getLogger(&#39;mapelites&#39;).level == logging.DEBUG else 0,
                                              max_iter=N_EPOCHS).fit(X=xs, y=ys)
                logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        else:
            raise ValueError(f&#39;Unrecognized estimator type: {self._estimator}&#39;)
        self._fitted = True
    
    def _extract_bin_context(self,
                             b: MAPBin) -&gt; npt.NDArray[np.float32]:
        return np.asarray(b.get_elite(population=&#39;feasible&#39;).representation)[CONTEXT_IDXS]
    
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        context = [self._extract_bin_context(b=b) for b in bins]
        return self.estimator.predict(X=context)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=self._extract_bin_context(b),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self.estimator = None
        self._fitted = False
    
    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;n_features_context&#39;: self._n_features_context,
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self._estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;ContextualBanditEmitter&#39;:
        re = ContextualBanditEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._n_features_context = my_args[&#39;n_features_context&#39;]
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re


class PreferenceBanditEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 0.9,
                 tau: float = 1.,
                 sampling_decay: float = 0.01,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a preference bandit emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;preference-bandit-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: Union[LinearRegression, NonLinearEstimator, MLPRegressor] = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        if self._estimator == &#39;linear&#39;:
            self.estimator = LinearRegression().fit(X=xs, y=ys)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        elif self._estimator == &#39;mlp&#39;:
            # if USE_TORCH:
            #     self.estimator = NonLinearEstimator(xshape=self._n_features_context,
            #                                         yshape=1)
            #     train_estimator(estimator=self._estimator,
            #                     xs=xs,
            #                     ys=ys,
            #                     n_epochs=20)
            # else:
                self.estimator = MLPRegressor(hidden_layer_sizes=(100, 100),
                                              activation=&#39;relu&#39;,
                                              alpha=1e-4,
                                              solver=&#39;lbfgs&#39;,
                                              verbose=1 if logging.getLogger(&#39;mapelites&#39;).level == logging.DEBUG else 0,
                                              max_iter=N_EPOCHS).fit(X=xs, y=ys)
                logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        else:
            raise ValueError(f&#39;Unrecognized estimator type: {self._estimator}&#39;)
        self._fitted = True
        
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;PreferenceBanditEmitter&#39;:
        re = PreferenceBanditEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re


class KNEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1) -&gt; None:
        &#34;&#34;&#34;Create a k-neighbours emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 9e-1.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to 1e-1.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;kn-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self.estimator: KNeighborsRegressor = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self._initial_epsilon: float = epsilon
        self.epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay

    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = KNeighborsRegressor(n_neighbors=5,
                                             leaf_size=30,
                                             weights=&#39;distance&#39;,
                                             p=2,
                                             metric=&#39;minkowski&#39;).fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins: List[MAPBin] = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self.estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = &#39;kn&#39;
        if isinstance(self.estimator, KNeighborsRegressor):
            # TODO: Save estimator parameters
            pass
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;KNEmitter&#39;:
        re = KNEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
                re._estimator = my_args[&#39;estimator_name&#39;]
                re.estimator = KNeighborsRegressor()
                # TODO: load parameters
        
        return re


class KernelEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 0.9,
                 tau: float = 1.,
                 sampling_decay: float = 0.01,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a kernel-based emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;rbf&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;kernel-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: KernelRidge = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
            return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = KernelRidge(kernel=self._estimator,
                                     alpha=1.0).fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
    
    def _predict(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, KernelRidge):
            # TODO: Save estimator parameters
            pass
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;KernelEmitter&#39;:
        re = KernelEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
                re._estimator = my_args[&#39;estimator_name&#39;]
                re.estimator = KernelRidge()
                # TODO: load parameters
        
        re._thompson_stats = my_args[&#39;thompson_stats&#39;]
        
        return re


class SimpleTabularEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;thompson&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a simple tabular emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name = &#39;simple-tabular-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: LinearRegression = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
        self.ts_priors = {}
        self._tot_actions = 0

    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=};{self.ts_priors=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = LinearRegression().fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
        
    def _predict(self,
                 idxs: List[Tuple[int, int]]) -&gt; Tuple[int, int]:
        return self.estimator.predict(idxs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([i, j]),
                                    y=1. if (i, j) in idxs else 0.)
                if (i, j) in self.ts_priors:
                    self.ts_priors[(i, j)][&#39;a&#39;] += 1 if (i, j) in idxs else 0
                    self.ts_priors[(i, j)][&#39;b&#39;] += 1
                else:
                    self.ts_priors[(i, j)] = {&#39;a&#39;: BETA_A + 1 if (i, j) in idxs else BETA_A,
                                              &#39;b&#39;: BETA_B}
        self._fit()
                
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(idxs=valid_idxs)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        elif self.sampling_strategy == &#39;thompson&#39;:
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {self.ts_priors=}&#39;)
            logits = [np.random.beta(a=self.ts_priors[idx][&#39;a&#39;] if idx in self.ts_priors else 1,
                                     b=self.ts_priors[idx][&#39;b&#39;] if idx in self.ts_priors else 1 + self._tot_actions,
                                     size=1) for idx in valid_idxs]
            valid_idxs = list(reversed([x for _, x in sorted(zip(logits, valid_idxs))]))
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        self._tot_actions += 1
        return selected_bins
    
    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False
        self.ts_priors = {}
        self._tot_actions = 0

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
            
            &#39;tot_actions&#39;: self._tot_actions,
            &#39;ts_priors&#39;: self.ts_priors
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;SimpleTabularEmitter&#39;:
        re = SimpleTabularEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        re._tot_actions = my_args[&#39;_tot_actions&#39;]
        re.ts_priors = my_args[&#39;ts_priors&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re
    

class HumanEmitter(Emitter):
    def __init__(self) -&gt; None:
        super().__init__()
        self.name = &#39;human-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        return []

    def reset(self) -&gt; None:
        pass    
    

def get_emitter_by_str(emitter: str) -&gt; Emitter:
    if emitter == &#39;random-emitter&#39;:
        return RandomEmitter()
    elif emitter == &#39;optimising-emitter&#39;:
        return OptimisingEmitter()
    elif emitter == &#39;optimising-emitter-v2&#39;:
        return OptimisingEmitterV2()
    else:
        raise NotImplementedError(f&#39;Unrecognized emitter from string: {emitter}&#39;)


emitters = {
    &#39;random-emitter&#39;: RandomEmitter,
    &#39;optimising-emitter&#39;: OptimisingEmitter,
    &#39;optimising-emitter-v2&#39;: OptimisingEmitterV2,
    &#39;greedy-emitter&#39;: GreedyEmitter,
    &#39;human-preference-matrix-emitter&#39;: HumanPrefMatrixEmitter,
    &#39;contextual-bandit-emitter&#39;: ContextualBanditEmitter,
    &#39;preference-bandit-emitter&#39;: PreferenceBanditEmitter,
    &#39;human-emitter&#39;: HumanEmitter,
    &#39;kn-emitter&#39;: KNEmitter,
    &#39;kernel-emitter&#39;: KernelEmitter,
}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pcgsepy.mapelites.emitters.get_emitter_by_str"><code class="name flex">
<span>def <span class="ident">get_emitter_by_str</span></span>(<span>emitter:str) ><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_emitter_by_str(emitter: str) -&gt; Emitter:
    if emitter == &#39;random-emitter&#39;:
        return RandomEmitter()
    elif emitter == &#39;optimising-emitter&#39;:
        return OptimisingEmitter()
    elif emitter == &#39;optimising-emitter-v2&#39;:
        return OptimisingEmitterV2()
    else:
        raise NotImplementedError(f&#39;Unrecognized emitter from string: {emitter}&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter"><code class="flex name class">
<span>class <span class="ident">ContextualBanditEmitter</span></span>
<span>(</span><span>n_features_context:int=3, sampling_strategy:str='gibbs', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.1, estimator:str='linear')</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a contextual bandit emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_features_context</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of features in the solution context. Defaults to len(CONTEXT_IDXS).</dd>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to 'gibbs'.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold value used if sampling strategy is 'epsilon-greedy'. Defaults to 9e-1.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to <code>1.</code>.</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to 1e-1.</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The estimator type. Valid values are 'linear' and 'mlp'. Defaults to 'linear'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ContextualBanditEmitter(Emitter):
    def __init__(self,
                 n_features_context: int = len(CONTEXT_IDXS),
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a contextual bandit emitter.

        Args:
            n_features_context (int, optional): The number of features in the solution context. Defaults to len(CONTEXT_IDXS).
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 9e-1.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to 1e-1.
            estimator (str, optional): The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;contextual-bandit-emitter&#39;
        self.requires_pre: bool = True
        
        self._n_features_context: int = n_features_context
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: Union[LinearRegression, NonLinearEstimator, MLPRegressor] = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self._initial_epsilon: float = epsilon
        self.epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        if self._estimator == &#39;linear&#39;:
            self.estimator = LinearRegression().fit(X=xs, y=ys)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        elif self._estimator == &#39;mlp&#39;:
            # if USE_TORCH:
            #     self.estimator = NonLinearEstimator(xshape=self._n_features_context,
            #                                         yshape=1)
            #     train_estimator(estimator=self._estimator,
            #                     xs=xs,
            #                     ys=ys,
            #                     n_epochs=20)
            # else:
                self.estimator = MLPRegressor(hidden_layer_sizes=(100, 100),
                                              activation=&#39;relu&#39;,
                                              alpha=1e-4,
                                              solver=&#39;lbfgs&#39;,
                                              verbose=1 if logging.getLogger(&#39;mapelites&#39;).level == logging.DEBUG else 0,
                                              max_iter=N_EPOCHS).fit(X=xs, y=ys)
                logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        else:
            raise ValueError(f&#39;Unrecognized estimator type: {self._estimator}&#39;)
        self._fitted = True
    
    def _extract_bin_context(self,
                             b: MAPBin) -&gt; npt.NDArray[np.float32]:
        return np.asarray(b.get_elite(population=&#39;feasible&#39;).representation)[CONTEXT_IDXS]
    
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        context = [self._extract_bin_context(b=b) for b in bins]
        return self.estimator.predict(X=context)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=self._extract_bin_context(b),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self.estimator = None
        self._fitted = False
    
    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;n_features_context&#39;: self._n_features_context,
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self._estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;ContextualBanditEmitter&#39;:
        re = ContextualBanditEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._n_features_context = my_args[&#39;n_features_context&#39;]
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter">ContextualBanditEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;ContextualBanditEmitter&#39;:
    re = ContextualBanditEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_decay = my_args[&#39;sampling_decay&#39;]
    re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
    re._n_features_context = my_args[&#39;n_features_context&#39;]
    re._fitted = my_args[&#39;fitted&#39;]
    
    if &#39;estimator_name&#39; in my_args.keys():
        # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
        #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
        if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
            re._estimator = &#39;linear&#39;
            re.estimator = LinearRegression()
            re.estimator.set_params(my_args[&#39;estimator_params&#39;])
            if my_args[&#39;estimator_coefs&#39;] is not None:
                re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
            if my_args[&#39;estimator_intercept&#39;] is not None:
                re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
        elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
            re._estimator = &#39;mlp&#39;
            re.estimator = MLPRegressor()
            re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
            re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
            re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
            re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
            re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
            re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
            re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
        else:
            raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
    
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
    selected_bins = []
    valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
    valid_bins = [bins[x] for x in valid_idxs]
    predicted_prefs = self._predict(bins=valid_bins)
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        logits = softmax(predicted_prefs / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
    for (i, j), b in np.ndenumerate(bins):
        if b.non_empty(pop=&#39;feasible&#39;):
            self._buffer.insert(x=self._extract_bin_context(b),
                                y=1. if (i, j) in idxs else 0.)
    self._fit()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self.epsilon = self._initial_epsilon
    self.tau = self._initial_tau
    self._buffer.clear()
    self.estimator = None
    self._fitted = False</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.ContextualBanditEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    j = {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;diversity_weight&#39;: self.diversity_weight,
        
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;sampling_decay&#39;: self.sampling_decay,
        &#39;buffer&#39;: self._buffer.to_json(),
        &#39;n_features_context&#39;: self._n_features_context,
        &#39;fitted&#39;: self._fitted,
    }
    j[&#39;estimator_name&#39;] = self._estimator
    if isinstance(self._estimator, LinearRegression):
        j[&#39;estimator_params&#39;] = self.estimator.get_params(),
        j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
        j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
    elif isinstance(self.estimator, MLPRegressor):
        j[&#39;coefs_&#39;] = self.estimator.coefs_
        j[&#39;intercepts_&#39;]: self.estimator.intercepts_
        j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
        j[&#39;n_iter_&#39;]: self.estimator.n_iter_
        j[&#39;n_layers_&#39;]: self.estimator.n_layers_
        j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
        j[&#39;out_activation_&#39;]: self.estimator.out_activation_
    # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
    #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
    return j</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.Emitter"><code class="flex name class">
<span>class <span class="ident">Emitter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Emitter(ABC):
    def __init__(self) -&gt; None:
        super().__init__()
        self.name = &#39;abstract-emitter&#39;
        self.requires_init = False
        self.requires_pre = False
        self.requires_post = False
        self.diversity_weight = 0.
    
    @abstractmethod
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        raise NotImplementedError(f&#39;The {self.name} must override the `pick_bin` method!&#39;)
    
    def init_emitter(self,
                     **kwargs) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `init_emitter` method!&#39;)
    
    def pre_step(self,
                 **kwargs) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `pre_step` method!&#39;)
    
    def post_step(self,
                 **kwargs):
        raise NotImplementedError(f&#39;The {self.name} must override the `post_step` method!&#39;)
    
    @abstractmethod
    def reset(self) -&gt; None:
        raise NotImplementedError(f&#39;The {self.name} must override the `reset` method!&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter">ContextualBanditEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.GreedyEmitter" href="#pcgsepy.mapelites.emitters.GreedyEmitter">GreedyEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.HumanEmitter" href="#pcgsepy.mapelites.emitters.HumanEmitter">HumanEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter">HumanPrefMatrixEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.KNEmitter" href="#pcgsepy.mapelites.emitters.KNEmitter">KNEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.KernelEmitter" href="#pcgsepy.mapelites.emitters.KernelEmitter">KernelEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.OptimisingEmitter" href="#pcgsepy.mapelites.emitters.OptimisingEmitter">OptimisingEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2">OptimisingEmitterV2</a></li>
<li><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter">PreferenceBanditEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.RandomEmitter" href="#pcgsepy.mapelites.emitters.RandomEmitter">RandomEmitter</a></li>
<li><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter">SimpleTabularEmitter</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.Emitter.init_emitter"><code class="name flex">
<span>def <span class="ident">init_emitter</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_emitter(self,
                 **kwargs) -&gt; None:
    raise NotImplementedError(f&#39;The {self.name} must override the `init_emitter` method!&#39;)</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.Emitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    raise NotImplementedError(f&#39;The {self.name} must override the `pick_bin` method!&#39;)</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.Emitter.post_step"><code class="name flex">
<span>def <span class="ident">post_step</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_step(self,
             **kwargs):
    raise NotImplementedError(f&#39;The {self.name} must override the `post_step` method!&#39;)</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.Emitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self,
             **kwargs) -&gt; None:
    raise NotImplementedError(f&#39;The {self.name} must override the `pre_step` method!&#39;)</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.Emitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def reset(self) -&gt; None:
    raise NotImplementedError(f&#39;The {self.name} must override the `reset` method!&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter"><code class="flex name class">
<span>class <span class="ident">GreedyEmitter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a greedy emitter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GreedyEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create a greedy emitter.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;greedy-emitter&#39;
        self.requires_pre = True
        self._last_selected: List[List[int]] = []
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        selected = [bins[idx] for idx in self._last_selected if bins[idx].non_empty(pop=&#39;feasible&#39;) or bins[idx].non_empty(pop=&#39;infeasbile&#39;)]
        return selected
    
    def reset(self) -&gt; None:
        self._last_selected = []

    def pre_step(self, **kwargs) -&gt; None:
        self._last_selected = []
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        for idx in idxs:
            self._last_selected.append(idx)
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight,
            &#39;last_selected&#39;: self._last_selected
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;GreedyEmitter&#39;:
        re = GreedyEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        re._last_selected = my_args[&#39;last_selected&#39;]
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.GreedyEmitter" href="#pcgsepy.mapelites.emitters.GreedyEmitter">GreedyEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;GreedyEmitter&#39;:
    re = GreedyEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    re.diversity_weight = my_args[&#39;diversity_weight&#39;]
    re._last_selected = my_args[&#39;last_selected&#39;]
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    selected = [bins[idx] for idx in self._last_selected if bins[idx].non_empty(pop=&#39;feasible&#39;) or bins[idx].non_empty(pop=&#39;infeasbile&#39;)]
    return selected</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    self._last_selected = []
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    for idx in idxs:
        self._last_selected.append(idx)</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self._last_selected = []</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.GreedyEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    return {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;diversity_weight&#39;: self.diversity_weight,
        &#39;last_selected&#39;: self._last_selected
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanEmitter"><code class="flex name class">
<span>class <span class="ident">HumanEmitter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HumanEmitter(Emitter):
    def __init__(self) -&gt; None:
        super().__init__()
        self.name = &#39;human-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        return []

    def reset(self) -&gt; None:
        pass    </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.HumanEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    return []</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    pass    </code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter"><code class="flex name class">
<span>class <span class="ident">HumanPrefMatrixEmitter</span></span>
<span>(</span><span>delta:float=1.0, decay:float=0.05, sampling_strategy:str='gibbs', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.1)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a human preference-matrix emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>delta</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The preference increment. Defaults to <code>1</code>.</dd>
<dt><strong><code>decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The preference decay. Defaults to <code>5e-2</code>.</dd>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to <code>gibbs</code>.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold used if sampling strategy is <code>epsilon-greedy</code>. Defaults to <code>9e-1</code>.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to <code>1.</code>.</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to <code>1e-1</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HumanPrefMatrixEmitter(Emitter):
    def __init__(self,
                 delta: float = 1.,
                 decay: float = 5e-2,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1) -&gt; None:
        &#34;&#34;&#34;Create a human preference-matrix emitter.

        Args:
            delta (float, optional): The preference increment. Defaults to `1`.
            decay (float, optional): The preference decay. Defaults to `5e-2`.
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to `gibbs`.
            epsilon (float, optional): The probability threshold used if sampling strategy is `epsilon-greedy`. Defaults to `9e-1`.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to `1e-1`.
        &#34;&#34;&#34;
        super().__init__()
        self.name = &#39;human-preference-matrix-emitter&#39;
        self.requires_init = True
        self.requires_post = True
        self.requires_pre = True
        
        self._delta = delta
        self._decay = decay
        self._prefs = None
        self._tot_actions = 0
        self._last_selected = []
        
        self.sampling_strategy = sampling_strategy  # epsilon_greedy or gibbs
        self.tau = tau
        self.epsilon = epsilon
        self._initial_tau = tau
        self._initial_epsilon = epsilon
        self.sampling_decay = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    def _build_pref_matrix(self,
                           bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; None:
        &#34;&#34;&#34;Build the preference matrix.

        Args:
            bins (np.ndarray[MAPBin]): The MAP-Elites bins.
        &#34;&#34;&#34;
        self._prefs = np.zeros(shape=bins.shape, dtype=np.float16)
        for (i, j), b in np.ndenumerate(bins):
            self._prefs[i, j] = self._delta if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasbile&#39;) else 0.
        
    def _get_n_new_bins(self,
                        bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; int:
        n_new_bins = 0
        for (_, _), b in np.ndenumerate(bins):
            css = [*b._feasible, *b._infeasible]
            for cs in css:
                if cs.age == CS_MAX_AGE:
                    n_new_bins += 1
                    break
        return n_new_bins
    
    def _reshape_matrix(self,
                        arr: np.typing.NDArray,
                        idx: Tuple[int, int]) -&gt; np.typing.NDArray:
        i, j = idx
        # create new matrix by coping preferences over to new column/rows
        # rows repetitions
        a = np.ones(shape=arr.shape[0], dtype=int)
        a[i] += 1
        # copy row
        arr = np.repeat(arr, repeats=a, axis=0)
        # columns repetitions
        a = np.ones(shape=arr.shape[1], dtype=int)
        a[j] += 1
        # copy column
        arr = np.repeat(arr, repeats=a, axis=1)
        return arr
    
    def _increase_preferences_res(self,
                                  idx: Tuple[int, int]) -&gt; None:
        assert self._prefs is not None, &#39;Human-preference emitter has not been initialized! Preference matrix has not been set.&#39;
        self._prefs = self._reshape_matrix(arr=self._prefs,
                                           idx=idx)
    
    def _decay_preferences(self) -&gt; None:
        self._prefs -= self._decay
        self._prefs[np.where(self._prefs &lt; 0)] = 0
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._prefs is not None, &#39;Human-preference emitter has not been initialized! Preference matrix has not been set.&#39;
        self._last_selected = []
        selected_bins = []
        non_empty = set([(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)])
        valid_prefs = set([tuple(x) for x in np.argwhere(self._prefs &gt; 0).tolist()])
        valid_idxs = list(non_empty.intersection(valid_prefs))
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: self._prefs[x], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            idxs = tuple(np.asarray(valid_idxs).transpose())
            logits = softmax(self._prefs[idxs] / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            valid_bins = bins[idxs]
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            self._last_selected.append(np.argwhere(bins == b).tolist()[0])
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins
    
    def init_emitter(self,
                     **kwargs) -&gt; None:
        assert self._prefs is None, f&#39;{self.name} has already been initialized!&#39;
        self._build_pref_matrix(bins=kwargs[&#39;bins&#39;])
        
    def pre_step(self, **kwargs) -&gt; None:
        assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
        bins: np.ndarray[MAPBin] = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        # get number of new/updated bins
        n_new_bins = self._get_n_new_bins(bins=bins)
        # update preference for selected bins
        for (i, j) in idxs:
            self._prefs[i, j] += 1.
            # if selected bin was just created, update parent bin accordingly
            if self._last_selected is not None:
                b = bins[i, j]
                css = [*b._feasible, *b._infeasible]
                for cs in css:
                    if cs.age == CS_MAX_AGE:
                        # we don&#39;t know which bin generated which, so update them all proportionally
                        for mn in self._last_selected:
                            self._prefs[mn[0], mn[1]] += 1 / n_new_bins
                        break
    
    def post_step(self,
                  bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; None:
        assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
        self._decay_preferences()

    def reset(self) -&gt; None:
        self._prefs = None
        self._tot_actions = 0
        self._tau = self._initial_tau
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;tot_actions&#39;: self._tot_actions,
            &#39;decay&#39;: self._decay,
            &#39;last_selected&#39;: self._last_selected,  # may need conversion tolist()
            &#39;prefs&#39;: self._prefs.tolist(),
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;sampling_strategy&#39;: self.sampling_strategy
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;HumanPrefMatrixEmitter&#39;:
        re = HumanPrefMatrixEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        re._tot_actions = my_args[&#39;tot_actions&#39;]
        re._decay = my_args[&#39;decay&#39;]
        re._last_selected = my_args[&#39;last_selected&#39;]  # may need conversion np.asarray
        re._prefs = np.asarray(my_args[&#39;prefs&#39;])
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_strategy = my_args[&#39;sampling_strategy&#39;]
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter">HumanPrefMatrixEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;HumanPrefMatrixEmitter&#39;:
    re = HumanPrefMatrixEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    re.diversity_weight = my_args[&#39;diversity_weight&#39;]
    re._tot_actions = my_args[&#39;tot_actions&#39;]
    re._decay = my_args[&#39;decay&#39;]
    re._last_selected = my_args[&#39;last_selected&#39;]  # may need conversion np.asarray
    re._prefs = np.asarray(my_args[&#39;prefs&#39;])
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_strategy = my_args[&#39;sampling_strategy&#39;]
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.init_emitter"><code class="name flex">
<span>def <span class="ident">init_emitter</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_emitter(self,
                 **kwargs) -&gt; None:
    assert self._prefs is None, f&#39;{self.name} has already been initialized!&#39;
    self._build_pref_matrix(bins=kwargs[&#39;bins&#39;])</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._prefs is not None, &#39;Human-preference emitter has not been initialized! Preference matrix has not been set.&#39;
    self._last_selected = []
    selected_bins = []
    non_empty = set([(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)])
    valid_prefs = set([tuple(x) for x in np.argwhere(self._prefs &gt; 0).tolist()])
    valid_idxs = list(non_empty.intersection(valid_prefs))
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: self._prefs[x], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        idxs = tuple(np.asarray(valid_idxs).transpose())
        logits = softmax(self._prefs[idxs] / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        valid_bins = bins[idxs]
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        self._last_selected.append(np.argwhere(bins == b).tolist()[0])
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.post_step"><code class="name flex">
<span>def <span class="ident">post_step</span></span>(<span>self, bins:np.ndarray[MAPBin])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_step(self,
              bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; None:
    assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
    self._decay_preferences()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    assert self._prefs is not None, f&#39;{self.name} has not been initialized! Preference matrix has not been set.&#39;
    bins: np.ndarray[MAPBin] = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    # get number of new/updated bins
    n_new_bins = self._get_n_new_bins(bins=bins)
    # update preference for selected bins
    for (i, j) in idxs:
        self._prefs[i, j] += 1.
        # if selected bin was just created, update parent bin accordingly
        if self._last_selected is not None:
            b = bins[i, j]
            css = [*b._feasible, *b._infeasible]
            for cs in css:
                if cs.age == CS_MAX_AGE:
                    # we don&#39;t know which bin generated which, so update them all proportionally
                    for mn in self._last_selected:
                        self._prefs[mn[0], mn[1]] += 1 / n_new_bins
                    break</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self._prefs = None
    self._tot_actions = 0
    self._tau = self._initial_tau</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    return {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;tot_actions&#39;: self._tot_actions,
        &#39;decay&#39;: self._decay,
        &#39;last_selected&#39;: self._last_selected,  # may need conversion tolist()
        &#39;prefs&#39;: self._prefs.tolist(),
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;sampling_strategy&#39;: self.sampling_strategy
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.KNEmitter"><code class="flex name class">
<span>class <span class="ident">KNEmitter</span></span>
<span>(</span><span>sampling_strategy:str='gibbs', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.1)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a k-neighbours emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to 'gibbs'.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold value used if sampling strategy is 'epsilon-greedy'. Defaults to 9e-1.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to <code>1.</code>.</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to 1e-1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KNEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1) -&gt; None:
        &#34;&#34;&#34;Create a k-neighbours emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 9e-1.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to `1.`.
            sampling_decay (float, optional): The sampling decay. Defaults to 1e-1.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;kn-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self.estimator: KNeighborsRegressor = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self._initial_epsilon: float = epsilon
        self.epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay

    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = KNeighborsRegressor(n_neighbors=5,
                                             leaf_size=30,
                                             weights=&#39;distance&#39;,
                                             p=2,
                                             metric=&#39;minkowski&#39;).fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins: List[MAPBin] = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self.estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = &#39;kn&#39;
        if isinstance(self.estimator, KNeighborsRegressor):
            # TODO: Save estimator parameters
            pass
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;KNEmitter&#39;:
        re = KNEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
                re._estimator = my_args[&#39;estimator_name&#39;]
                re.estimator = KNeighborsRegressor()
                # TODO: load parameters
        
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.KNEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.KNEmitter" href="#pcgsepy.mapelites.emitters.KNEmitter">KNEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;KNEmitter&#39;:
    re = KNEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_decay = my_args[&#39;sampling_decay&#39;]
    re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
    re._fitted = my_args[&#39;fitted&#39;]
    
    if &#39;estimator_name&#39; in my_args.keys():
            re._estimator = my_args[&#39;estimator_name&#39;]
            re.estimator = KNeighborsRegressor()
            # TODO: load parameters
    
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.KNEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
    selected_bins = []
    valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
    valid_bins: List[MAPBin] = [bins[x] for x in valid_idxs]        
    predicted_prefs = self._predict(bins=valid_bins)
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        logits = softmax(predicted_prefs / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KNEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
    bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
    bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
    for (i, j), b in np.ndenumerate(bins):
        if b.non_empty(pop=&#39;feasible&#39;):
            self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                y=1. if (i, j) in idxs else 0.)
    self._fit()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KNEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self.epsilon = self._initial_epsilon
    self.tau = self._initial_tau
    self._buffer.clear()
    self.estimator = None
    self._fitted = False</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KNEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    j = {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;sampling_decay&#39;: self.sampling_decay,
        &#39;buffer&#39;: self._buffer.to_json(),
        &#39;fitted&#39;: self._fitted,
    }
    j[&#39;estimator_name&#39;] = &#39;kn&#39;
    if isinstance(self.estimator, KNeighborsRegressor):
        # TODO: Save estimator parameters
        pass
    
    return j</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter"><code class="flex name class">
<span>class <span class="ident">KernelEmitter</span></span>
<span>(</span><span>sampling_strategy:str='gibbs', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.01, estimator:str='linear')</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a kernel-based emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to 'gibbs'.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold value used if sampling strategy is 'epsilon-greedy'. Defaults to 0.9.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to 1..</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to 0.01.</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The estimator type. Valid values are 'linear' and 'rbf'. Defaults to 'linear'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KernelEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 0.9,
                 tau: float = 1.,
                 sampling_decay: float = 0.01,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a kernel-based emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;rbf&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;kernel-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: KernelRidge = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
            return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = KernelRidge(kernel=self._estimator,
                                     alpha=1.0).fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
    
    def _predict(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, KernelRidge):
            # TODO: Save estimator parameters
            pass
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;KernelEmitter&#39;:
        re = KernelEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
                re._estimator = my_args[&#39;estimator_name&#39;]
                re.estimator = KernelRidge()
                # TODO: load parameters
        
        re._thompson_stats = my_args[&#39;thompson_stats&#39;]
        
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.KernelEmitter" href="#pcgsepy.mapelites.emitters.KernelEmitter">KernelEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;KernelEmitter&#39;:
    re = KernelEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_decay = my_args[&#39;sampling_decay&#39;]
    re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
    re._fitted = my_args[&#39;fitted&#39;]
    
    if &#39;estimator_name&#39; in my_args.keys():
            re._estimator = my_args[&#39;estimator_name&#39;]
            re.estimator = KernelRidge()
            # TODO: load parameters
    
    re._thompson_stats = my_args[&#39;thompson_stats&#39;]
    
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
    selected_bins = []
    valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
    valid_bins = [bins[x] for x in valid_idxs]        
    predicted_prefs = self._predict(bins=valid_bins)
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        logits = softmax(predicted_prefs / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
    bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
    bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
    for (i, j), b in np.ndenumerate(bins):
        if b.non_empty(pop=&#39;feasible&#39;):
            self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                y=1. if (i, j) in idxs else 0.)
    self._fit()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self.epsilon = self._initial_epsilon
    self.tau = self._initial_tau
    self._buffer.clear()
    self._estimator = None
    self._fitted = False</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.KernelEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    j = {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;sampling_decay&#39;: self.sampling_decay,
        &#39;buffer&#39;: self._buffer.to_json(),
        &#39;fitted&#39;: self._fitted,
    }
    j[&#39;estimator_name&#39;] = self._estimator
    if isinstance(self.estimator, KernelRidge):
        # TODO: Save estimator parameters
        pass
    
    return j</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.NonLinearEstimator"><code class="flex name class">
<span>class <span class="ident">NonLinearEstimator</span></span>
<span>(</span><span>xshape, yshape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NonLinearEstimator():
    def __init__(self,
                 xshape,
                 yshape):
        raise NotImplementedError(&#39;This object should never be instantiated&#39;)

    def train_estimator(estimator, xs, ys, n_epochs):
        raise NotImplementedError(&#39;This function should never be called&#39;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.NonLinearEstimator.train_estimator"><code class="name flex">
<span>def <span class="ident">train_estimator</span></span>(<span>estimator, xs, ys, n_epochs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_estimator(estimator, xs, ys, n_epochs):
    raise NotImplementedError(&#39;This function should never be called&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitter"><code class="flex name class">
<span>class <span class="ident">OptimisingEmitter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create an optimising emitter.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OptimisingEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create an optimising emitter.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;optimising-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The selected bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        sorted_bins = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
        fcs, ics = 0, 0
        selected = []
        while fcs &lt; 2 or ics &lt; 2:
            selected.append(sorted_bins.pop(0))
            fcs += len(selected[-1]._feasible)
            ics += len(selected[-1]._infeasible)
        return selected

    def reset(self) -&gt; None:
        pass
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitter&#39;:
        re = OptimisingEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.OptimisingEmitter" href="#pcgsepy.mapelites.emitters.OptimisingEmitter">OptimisingEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitter&#39;:
    re = OptimisingEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    re.diversity_weight = my_args[&#39;diversity_weight&#39;]
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Select the bin whose elite content has the highest feasible fitness.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>List[MAPBin]</code></dt>
<dd>The list of valid bins.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>MAPBin</code></dt>
<dd>The selected bin.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

    Args:
        bins (List[MAPBin]): The list of valid bins.

    Returns:
        MAPBin: The selected bin.
    &#34;&#34;&#34;
    bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
    sorted_bins = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
    fcs, ics = 0, 0
    selected = []
    while fcs &lt; 2 or ics &lt; 2:
        selected.append(sorted_bins.pop(0))
        fcs += len(selected[-1]._feasible)
        ics += len(selected[-1]._infeasible)
    return selected</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    return {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;diversity_weight&#39;: self.diversity_weight
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitterV2"><code class="flex name class">
<span>class <span class="ident">OptimisingEmitterV2</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create an optimising emitter (population-based).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OptimisingEmitterV2(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create an optimising emitter (population-based).&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;optimising-emitter-v2&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[List[MAPBin]]:
        &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The selected bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        sorted_bins_f = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
        sorted_bins_i = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;infeasible&#39;), reverse=True)
        fcs, ics = 0, 0
        selected = [[], []]
        while fcs &lt; 2:
            selected[0].append(sorted_bins_f.pop(0))
            fcs += len(selected[0][-1]._feasible)
        while ics &lt; 2:
            selected[1].append(sorted_bins_i.pop(0))
            ics += len(selected[1][-1]._infeasible)
        return selected

    def reset(self) -&gt; None:
        pass
    
    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitterV2&#39;:
        re = OptimisingEmitterV2()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitterV2.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2">OptimisingEmitterV2</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;OptimisingEmitterV2&#39;:
    re = OptimisingEmitterV2()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    re.diversity_weight = my_args[&#39;diversity_weight&#39;]
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitterV2.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>Select the bin whose elite content has the highest feasible fitness.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>List[MAPBin]</code></dt>
<dd>The list of valid bins.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>MAPBin</code></dt>
<dd>The selected bin.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[List[MAPBin]]:
    &#34;&#34;&#34;Select the bin whose elite content has the highest feasible fitness.

    Args:
        bins (List[MAPBin]): The list of valid bins.

    Returns:
        MAPBin: The selected bin.
    &#34;&#34;&#34;
    bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
    sorted_bins_f = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;feasible&#39;), reverse=True)
    sorted_bins_i = sorted(bins, key=lambda x: x.get_metric(metric=&#39;fitness&#39;, use_mean=True, population=&#39;infeasible&#39;), reverse=True)
    fcs, ics = 0, 0
    selected = [[], []]
    while fcs &lt; 2:
        selected[0].append(sorted_bins_f.pop(0))
        fcs += len(selected[0][-1]._feasible)
    while ics &lt; 2:
        selected[1].append(sorted_bins_i.pop(0))
        ics += len(selected[1][-1]._infeasible)
    return selected</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitterV2.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.OptimisingEmitterV2.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    return {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;diversity_weight&#39;: self.diversity_weight
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter"><code class="flex name class">
<span>class <span class="ident">PreferenceBanditEmitter</span></span>
<span>(</span><span>sampling_strategy:str='gibbs', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.01, estimator:str='linear')</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a preference bandit emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to 'gibbs'.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold value used if sampling strategy is 'epsilon-greedy'. Defaults to 0.9.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to 1..</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to 0.01.</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The estimator type. Valid values are 'linear' and 'mlp'. Defaults to 'linear'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PreferenceBanditEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;gibbs&#39;,
                 epsilon: float = 0.9,
                 tau: float = 1.,
                 sampling_decay: float = 0.01,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a preference bandit emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name: str = &#39;preference-bandit-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: Union[LinearRegression, NonLinearEstimator, MLPRegressor] = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
    
    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        if self._estimator == &#39;linear&#39;:
            self.estimator = LinearRegression().fit(X=xs, y=ys)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        elif self._estimator == &#39;mlp&#39;:
            # if USE_TORCH:
            #     self.estimator = NonLinearEstimator(xshape=self._n_features_context,
            #                                         yshape=1)
            #     train_estimator(estimator=self._estimator,
            #                     xs=xs,
            #                     ys=ys,
            #                     n_epochs=20)
            # else:
                self.estimator = MLPRegressor(hidden_layer_sizes=(100, 100),
                                              activation=&#39;relu&#39;,
                                              alpha=1e-4,
                                              solver=&#39;lbfgs&#39;,
                                              verbose=1 if logging.getLogger(&#39;mapelites&#39;).level == logging.DEBUG else 0,
                                              max_iter=N_EPOCHS).fit(X=xs, y=ys)
                logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        else:
            raise ValueError(f&#39;Unrecognized estimator type: {self._estimator}&#39;)
        self._fitted = True
        
    def _predict(self,
                 bins: List[MAPBin]) -&gt; Tuple[int, int]:
        xs = np.asarray([b.get_elite(population=&#39;feasible&#39;).b_descs for b in bins])
        return self.estimator.predict(xs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
        bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                    y=1. if (i, j) in idxs else 0.)
        self._fit()
        
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(bins=valid_bins)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        return selected_bins

    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;PreferenceBanditEmitter&#39;:
        re = PreferenceBanditEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter">PreferenceBanditEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;PreferenceBanditEmitter&#39;:
    re = PreferenceBanditEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_decay = my_args[&#39;sampling_decay&#39;]
    re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
    re._fitted = my_args[&#39;fitted&#39;]
    
    if &#39;estimator_name&#39; in my_args.keys():
        # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
        #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
        if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
            re._estimator = &#39;linear&#39;
            re.estimator = LinearRegression()
            re.estimator.set_params(my_args[&#39;estimator_params&#39;])
            if my_args[&#39;estimator_coefs&#39;] is not None:
                re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
            if my_args[&#39;estimator_intercept&#39;] is not None:
                re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
        elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
            re._estimator = &#39;mlp&#39;
            re.estimator = MLPRegressor()
            re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
            re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
            re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
            re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
            re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
            re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
            re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
        else:
            raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
    
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
    selected_bins = []
    valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
    valid_bins = [bins[x] for x in valid_idxs]        
    predicted_prefs = self._predict(bins=valid_bins)
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        logits = softmax(predicted_prefs / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
    bcs0 = np.cumsum([b.bin_size[0] for b in bins[0, :]])[:-1]
    bcs1 = np.cumsum([b.bin_size[1] for b in bins[:, 0]])[:-1]
    for (i, j), b in np.ndenumerate(bins):
        if b.non_empty(pop=&#39;feasible&#39;):
            self._buffer.insert(x=np.asarray([bcs0[i], bcs1[j]]),
                                y=1. if (i, j) in idxs else 0.)
    self._fit()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self.epsilon = self._initial_epsilon
    self.tau = self._initial_tau
    self._buffer.clear()
    self._estimator = None
    self._fitted = False</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    j = {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;sampling_decay&#39;: self.sampling_decay,
        &#39;buffer&#39;: self._buffer.to_json(),
        &#39;fitted&#39;: self._fitted,
    }
    j[&#39;estimator_name&#39;] = self._estimator
    if isinstance(self.estimator, LinearRegression):
        j[&#39;estimator_params&#39;] = self.estimator.get_params(),
        j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
        j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
    elif isinstance(self.estimator, MLPRegressor):
        j[&#39;coefs_&#39;] = self.estimator.coefs_
        j[&#39;intercepts_&#39;]: self.estimator.intercepts_
        j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
        j[&#39;n_iter_&#39;]: self.estimator.n_iter_
        j[&#39;n_layers_&#39;]: self.estimator.n_layers_
        j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
        j[&#39;out_activation_&#39;]: self.estimator.out_activation_
    # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
    #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
    
    return j</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.RandomEmitter"><code class="flex name class">
<span>class <span class="ident">RandomEmitter</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a random emitter class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomEmitter(Emitter):
    def __init__(self) -&gt; None:
        &#34;&#34;&#34;Create a random emitter class.&#34;&#34;&#34;
        super().__init__()
        self.name = &#39;random-emitter&#39;
    
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        &#34;&#34;&#34;Randomly return a bin among possible valid bins.

        Args:
            bins (List[MAPBin]): The list of valid bins.

        Returns:
            MAPBin: The randomly picked bin.
        &#34;&#34;&#34;
        bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
        fcs, ics = 0, 0
        selected = []
        while fcs &lt; 2 or ics &lt; 2:
            selected.append(bins.pop(np.random.choice(np.arange(len(bins)))))
            fcs += len(selected[-1]._feasible)
            ics += len(selected[-1]._infeasible)
        return selected
    
    def reset(self) -&gt; None:
        pass

    def to_json(self) -&gt; Dict[str, Any]:
        return {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            &#39;diversity_weight&#39;: self.diversity_weight
        }
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;RandomEmitter&#39;:
        re = RandomEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        re.diversity_weight = my_args[&#39;diversity_weight&#39;]
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.RandomEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.RandomEmitter" href="#pcgsepy.mapelites.emitters.RandomEmitter">RandomEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;RandomEmitter&#39;:
    re = RandomEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    re.diversity_weight = my_args[&#39;diversity_weight&#39;]
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.RandomEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly return a bin among possible valid bins.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>bins</code></strong> :&ensp;<code>List[MAPBin]</code></dt>
<dd>The list of valid bins.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>MAPBin</code></dt>
<dd>The randomly picked bin.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    &#34;&#34;&#34;Randomly return a bin among possible valid bins.

    Args:
        bins (List[MAPBin]): The list of valid bins.

    Returns:
        MAPBin: The randomly picked bin.
    &#34;&#34;&#34;
    bins = [b for b in bins.flatten().tolist() if b.non_empty(pop=&#39;feasible&#39;) or b.non_empty(pop=&#39;infeasible&#39;)]
    fcs, ics = 0, 0
    selected = []
    while fcs &lt; 2 or ics &lt; 2:
        selected.append(bins.pop(np.random.choice(np.arange(len(bins)))))
        fcs += len(selected[-1]._feasible)
        ics += len(selected[-1]._infeasible)
    return selected</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.RandomEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    pass</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.RandomEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    return {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        &#39;diversity_weight&#39;: self.diversity_weight
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter"><code class="flex name class">
<span>class <span class="ident">SimpleTabularEmitter</span></span>
<span>(</span><span>sampling_strategy:str='thompson', epsilon:float=0.9, tau:float=1.0, sampling_decay:float=0.1, estimator:str='linear')</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Create a simple tabular emitter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sampling_strategy</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The sampling strategy. Valid values are <code>epsilon-greedy</code> and <code>gibbs</code>. Defaults to 'gibbs'.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The probability threshold value used if sampling strategy is 'epsilon-greedy'. Defaults to 0.9.</dd>
<dt><strong><code>tau</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The temperature used if sampling strategy is <code>gibbs</code>. Defaults to 1..</dd>
<dt><strong><code>sampling_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The sampling decay. Defaults to 0.01.</dd>
<dt><strong><code>estimator</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The estimator type. Valid values are 'linear' and 'mlp'. Defaults to 'linear'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SimpleTabularEmitter(Emitter):
    def __init__(self,
                 sampling_strategy: str = &#39;thompson&#39;,
                 epsilon: float = 9e-1,
                 tau: float = 1.,
                 sampling_decay: float = 1e-1,
                 estimator: str = &#39;linear&#39;) -&gt; None:
        &#34;&#34;&#34;Create a simple tabular emitter.

        Args:
            sampling_strategy (str, optional): The sampling strategy. Valid values are `epsilon-greedy` and `gibbs`. Defaults to &#39;gibbs&#39;.
            epsilon (float, optional): The probability threshold value used if sampling strategy is &#39;epsilon-greedy&#39;. Defaults to 0.9.
            tau (float, optional): The temperature used if sampling strategy is `gibbs`. Defaults to 1..
            sampling_decay (float, optional): The sampling decay. Defaults to 0.01.
            estimator (str, optional):  The estimator type. Valid values are &#39;linear&#39; and &#39;mlp&#39;. Defaults to &#39;linear&#39;.
        &#34;&#34;&#34;
        super().__init__()
        self.name = &#39;simple-tabular-emitter&#39;
        self.requires_pre: bool = True
        
        self._buffer: Buffer = Buffer(merge_method=mean_merge)
        self._estimator: str = estimator
        self.estimator: LinearRegression = None
        self._fitted: bool = False
        
        self.sampling_strategy: str = sampling_strategy
        self.tau: float = tau
        self._initial_tau: float = tau
        self.epsilon: float = epsilon
        self._initial_epsilon: float = epsilon
        self.sampling_decay: float = sampling_decay
        self.ts_priors = {}
        self._tot_actions = 0

    def __repr__(self) -&gt; str:
        return f&#39;{self.name} {self._estimator} {self.sampling_strategy} ({self.tau=};{self.epsilon=};{self.ts_priors=})&#39;
    
    @ignore_warnings(category=ConvergenceWarning)
    def _fit(self) -&gt; None:
        xs, ys = self._buffer.get()
        self.estimator = LinearRegression().fit(X=xs, y=ys)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}._fit] datapoints={len(xs)}; nonzero_count={len(np.nonzero(ys)[0])}; estimator_score={self.estimator.score(xs, ys):.2%}&#39;)
        self._fitted = True
        
    def _predict(self,
                 idxs: List[Tuple[int, int]]) -&gt; Tuple[int, int]:
        return self.estimator.predict(idxs)
    
    def pre_step(self, **kwargs) -&gt; None:
        bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
        idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
        for (i, j), b in np.ndenumerate(bins):
            if b.non_empty(pop=&#39;feasible&#39;):
                self._buffer.insert(x=np.asarray([i, j]),
                                    y=1. if (i, j) in idxs else 0.)
                if (i, j) in self.ts_priors:
                    self.ts_priors[(i, j)][&#39;a&#39;] += 1 if (i, j) in idxs else 0
                    self.ts_priors[(i, j)][&#39;b&#39;] += 1
                else:
                    self.ts_priors[(i, j)] = {&#39;a&#39;: BETA_A + 1 if (i, j) in idxs else BETA_A,
                                              &#39;b&#39;: BETA_B}
        self._fit()
                
    def pick_bin(self,
                 bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
        assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
        selected_bins = []
        valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
        valid_bins = [bins[x] for x in valid_idxs]        
        predicted_prefs = self._predict(idxs=valid_idxs)
        if self.sampling_strategy == &#39;epsilon-greedy&#39;:
            p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
            self.epsilon -= self.sampling_decay * self.epsilon
            if p:
                np.random.shuffle(valid_idxs)
            else:
                valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        elif self.sampling_strategy == &#39;gibbs&#39;:
            logits = softmax(predicted_prefs / self.tau)
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
            self.tau -= self.sampling_decay * self.tau
            sampled_bins = np.random.choice(valid_bins,
                                            size=len(valid_bins),
                                            replace=False,
                                            p=logits)
        elif self.sampling_strategy == &#39;thompson&#39;:
            logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {self.ts_priors=}&#39;)
            logits = [np.random.beta(a=self.ts_priors[idx][&#39;a&#39;] if idx in self.ts_priors else 1,
                                     b=self.ts_priors[idx][&#39;b&#39;] if idx in self.ts_priors else 1 + self._tot_actions,
                                     size=1) for idx in valid_idxs]
            valid_idxs = list(reversed([x for _, x in sorted(zip(logits, valid_idxs))]))
            sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
        else:
            raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
        fcs, ics = 0, 0
        for b in sampled_bins:
            fcs += len(b._feasible)
            ics += len(b._infeasible)
            selected_bins.append(b)
            if fcs &gt; 0 and ics &gt; 0:
                break
        self._tot_actions += 1
        return selected_bins
    
    def reset(self) -&gt; None:
        self.epsilon = self._initial_epsilon
        self.tau = self._initial_tau
        self._buffer.clear()
        self._estimator = None
        self._fitted = False
        self.ts_priors = {}
        self._tot_actions = 0

    def to_json(self) -&gt; Dict[str, Any]:
        j = {
            &#39;name&#39;: self.name,
            &#39;requires_init&#39;: self.requires_init,
            &#39;requires_pre&#39;: self.requires_pre,
            &#39;requires_post&#39;: self.requires_post,
            
            &#39;initial_epsilon&#39;: self._initial_epsilon,
            &#39;epsilon&#39;: self.epsilon,
            &#39;initial_tau&#39;: self._initial_tau,
            &#39;tau&#39;: self.tau,
            &#39;sampling_decay&#39;: self.sampling_decay,
            &#39;buffer&#39;: self._buffer.to_json(),
            &#39;fitted&#39;: self._fitted,
            
            &#39;tot_actions&#39;: self._tot_actions,
            &#39;ts_priors&#39;: self.ts_priors
        }
        j[&#39;estimator_name&#39;] = self._estimator
        if isinstance(self.estimator, LinearRegression):
            j[&#39;estimator_params&#39;] = self.estimator.get_params(),
            j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
            j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
        elif isinstance(self.estimator, MLPRegressor):
            j[&#39;coefs_&#39;] = self.estimator.coefs_
            j[&#39;intercepts_&#39;]: self.estimator.intercepts_
            j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
            j[&#39;n_iter_&#39;]: self.estimator.n_iter_
            j[&#39;n_layers_&#39;]: self.estimator.n_layers_
            j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
            j[&#39;out_activation_&#39;]: self.estimator.out_activation_
        # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
        #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
        
        return j
    
    @staticmethod
    def from_json(my_args: Dict[str, Any]) -&gt; &#39;SimpleTabularEmitter&#39;:
        re = SimpleTabularEmitter()
        re.name = my_args[&#39;name&#39;]
        re.requires_init = my_args[&#39;requires_init&#39;]
        re.requires_pre = my_args[&#39;requires_pre&#39;]
        re.requires_post = my_args[&#39;requires_post&#39;]
        
        re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
        re._initial_tau = my_args[&#39;initial_tau&#39;]
        re.epsilon = my_args[&#39;epsilon&#39;]
        re.tau = my_args[&#39;tau&#39;]
        re.sampling_decay = my_args[&#39;sampling_decay&#39;]
        re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
        re._fitted = my_args[&#39;fitted&#39;]
        
        re._tot_actions = my_args[&#39;_tot_actions&#39;]
        re.ts_priors = my_args[&#39;ts_priors&#39;]
        
        if &#39;estimator_name&#39; in my_args.keys():
            # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
            #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
            if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
                re._estimator = &#39;linear&#39;
                re.estimator = LinearRegression()
                re.estimator.set_params(my_args[&#39;estimator_params&#39;])
                if my_args[&#39;estimator_coefs&#39;] is not None:
                    re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
                if my_args[&#39;estimator_intercept&#39;] is not None:
                    re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
            elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
                re._estimator = &#39;mlp&#39;
                re.estimator = MLPRegressor()
                re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
                re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
                re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
                re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
                re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
                re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
                re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
            else:
                raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
        
        return re</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>my_args:Dict[str,Any]) ><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter">SimpleTabularEmitter</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(my_args: Dict[str, Any]) -&gt; &#39;SimpleTabularEmitter&#39;:
    re = SimpleTabularEmitter()
    re.name = my_args[&#39;name&#39;]
    re.requires_init = my_args[&#39;requires_init&#39;]
    re.requires_pre = my_args[&#39;requires_pre&#39;]
    re.requires_post = my_args[&#39;requires_post&#39;]
    
    re._initial_epsilon = my_args[&#39;initial_epsilon&#39;]
    re._initial_tau = my_args[&#39;initial_tau&#39;]
    re.epsilon = my_args[&#39;epsilon&#39;]
    re.tau = my_args[&#39;tau&#39;]
    re.sampling_decay = my_args[&#39;sampling_decay&#39;]
    re.buffer = Buffer.from_json(my_args[&#39;buffer&#39;])
    re._fitted = my_args[&#39;fitted&#39;]
    
    re._tot_actions = my_args[&#39;_tot_actions&#39;]
    re.ts_priors = my_args[&#39;ts_priors&#39;]
    
    if &#39;estimator_name&#39; in my_args.keys():
        # if my_args[&#39;estimator_name&#39;] == &#39;NonLinearEstimator&#39; and USE_TORCH and not USE_LINEAR_ESTIMATOR:
        #     re._estimator = NonLinearEstimator.from_json(my_args=my_args[&#39;estimator_parameters&#39;])
        if my_args[&#39;estimator_name&#39;] == &#39;linear&#39;:
            re._estimator = &#39;linear&#39;
            re.estimator = LinearRegression()
            re.estimator.set_params(my_args[&#39;estimator_params&#39;])
            if my_args[&#39;estimator_coefs&#39;] is not None:
                re.estimator.coef_ = np.asarray(my_args[&#39;estimator_coefs&#39;])
            if my_args[&#39;estimator_intercept&#39;] is not None:
                re.estimator.intercept_ = np.asarray(my_args[&#39;estimator_intercept&#39;])
        elif my_args[&#39;estimator_name&#39;] == &#39;mlp&#39;:
            re._estimator = &#39;mlp&#39;
            re.estimator = MLPRegressor()
            re.estimator.coefs_ = my_args[&#39;coefs_&#39;]
            re.estimator.intercepts_ = my_args[&#39;intercepts_&#39;]
            re.estimator.n_features_in_ = my_args[&#39;n_features_in_&#39;]
            re.estimator.n_iter_ = my_args[&#39;n_iter_&#39;]
            re.estimator.n_layers_ = my_args[&#39;n_layers_&#39;]
            re.estimator.n_outputs_ = my_args[&#39;n_outputs_&#39;]
            re.estimator.out_activation_ = my_args[&#39;out_activation_&#39;]
        else:
            raise ValueError(f&#39;Unrecognized estimator name: {my_args[&#34;estimator_name&#34;]}.&#39;)
    
    return re</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter.pick_bin"><code class="name flex">
<span>def <span class="ident">pick_bin</span></span>(<span>self, bins:np.ndarray[MAPBin]) >List[<a title="pcgsepy.mapelites.bin.MAPBin" href="bin.html#pcgsepy.mapelites.bin.MAPBin">MAPBin</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_bin(self,
             bins: &#39;np.ndarray[MAPBin]&#39;) -&gt; List[MAPBin]:
    assert self._fitted, f&#39;{self.name} requires fitting and has not been fit yet!&#39;
    selected_bins = []
    valid_idxs = [(i, j) for (i, j), b in np.ndenumerate(bins) if b.non_empty(&#39;feasible&#39;) and b.non_empty(&#39;infeasible&#39;)]
    valid_bins = [bins[x] for x in valid_idxs]        
    predicted_prefs = self._predict(idxs=valid_idxs)
    if self.sampling_strategy == &#39;epsilon-greedy&#39;:
        p = (np.random.uniform(low=0, high=1, size=1) &lt; self.epsilon)[0]
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {p=}&#39;)
        self.epsilon -= self.sampling_decay * self.epsilon
        if p:
            np.random.shuffle(valid_idxs)
        else:
            valid_idxs.sort(key=lambda x: predicted_prefs[np.argwhere(valid_idxs == x)], reverse=True)
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    elif self.sampling_strategy == &#39;gibbs&#39;:
        logits = softmax(predicted_prefs / self.tau)
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {logits=}&#39;)
        self.tau -= self.sampling_decay * self.tau
        sampled_bins = np.random.choice(valid_bins,
                                        size=len(valid_bins),
                                        replace=False,
                                        p=logits)
    elif self.sampling_strategy == &#39;thompson&#39;:
        logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pick_bin] {self.ts_priors=}&#39;)
        logits = [np.random.beta(a=self.ts_priors[idx][&#39;a&#39;] if idx in self.ts_priors else 1,
                                 b=self.ts_priors[idx][&#39;b&#39;] if idx in self.ts_priors else 1 + self._tot_actions,
                                 size=1) for idx in valid_idxs]
        valid_idxs = list(reversed([x for _, x in sorted(zip(logits, valid_idxs))]))
        sampled_bins = bins[tuple(np.asarray(valid_idxs).transpose())]
    else:
        raise Exception(f&#39;Unknown sampling method for emitter: {self.sampling_strategy}.&#39;)
    fcs, ics = 0, 0
    for b in sampled_bins:
        fcs += len(b._feasible)
        ics += len(b._infeasible)
        selected_bins.append(b)
        if fcs &gt; 0 and ics &gt; 0:
            break
    self._tot_actions += 1
    return selected_bins</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter.pre_step"><code class="name flex">
<span>def <span class="ident">pre_step</span></span>(<span>self, **kwargs) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_step(self, **kwargs) -&gt; None:
    bins: &#39;np.ndarray[MAPBin]&#39; = kwargs[&#39;bins&#39;]
    idxs: List[Tuple[int, int]] = [*kwargs[&#39;selected_idxs&#39;], *kwargs[&#39;expanded_idxs&#39;]]
    logging.getLogger(&#39;mapelites&#39;).debug(f&#39;[{__name__}.pre_step] {idxs=}&#39;)
    for (i, j), b in np.ndenumerate(bins):
        if b.non_empty(pop=&#39;feasible&#39;):
            self._buffer.insert(x=np.asarray([i, j]),
                                y=1. if (i, j) in idxs else 0.)
            if (i, j) in self.ts_priors:
                self.ts_priors[(i, j)][&#39;a&#39;] += 1 if (i, j) in idxs else 0
                self.ts_priors[(i, j)][&#39;b&#39;] += 1
            else:
                self.ts_priors[(i, j)] = {&#39;a&#39;: BETA_A + 1 if (i, j) in idxs else BETA_A,
                                          &#39;b&#39;: BETA_B}
    self._fit()</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    self.epsilon = self._initial_epsilon
    self.tau = self._initial_tau
    self._buffer.clear()
    self._estimator = None
    self._fitted = False
    self.ts_priors = {}
    self._tot_actions = 0</code></pre>
</details>
</dd>
<dt id="pcgsepy.mapelites.emitters.SimpleTabularEmitter.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) >Dict[str,Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; Dict[str, Any]:
    j = {
        &#39;name&#39;: self.name,
        &#39;requires_init&#39;: self.requires_init,
        &#39;requires_pre&#39;: self.requires_pre,
        &#39;requires_post&#39;: self.requires_post,
        
        &#39;initial_epsilon&#39;: self._initial_epsilon,
        &#39;epsilon&#39;: self.epsilon,
        &#39;initial_tau&#39;: self._initial_tau,
        &#39;tau&#39;: self.tau,
        &#39;sampling_decay&#39;: self.sampling_decay,
        &#39;buffer&#39;: self._buffer.to_json(),
        &#39;fitted&#39;: self._fitted,
        
        &#39;tot_actions&#39;: self._tot_actions,
        &#39;ts_priors&#39;: self.ts_priors
    }
    j[&#39;estimator_name&#39;] = self._estimator
    if isinstance(self.estimator, LinearRegression):
        j[&#39;estimator_params&#39;] = self.estimator.get_params(),
        j[&#39;estimator_coefs&#39;] = self.estimator.coef_.tolist() if self._fitted else None,
        j[&#39;estimator_intercept&#39;] = np.asarray(self.estimator.intercept_).tolist() if self._fitted else None,
    elif isinstance(self.estimator, MLPRegressor):
        j[&#39;coefs_&#39;] = self.estimator.coefs_
        j[&#39;intercepts_&#39;]: self.estimator.intercepts_
        j[&#39;n_features_in_&#39;]: self.estimator.n_features_in_
        j[&#39;n_iter_&#39;]: self.estimator.n_iter_
        j[&#39;n_layers_&#39;]: self.estimator.n_layers_
        j[&#39;n_outputs_&#39;]: self.estimator.n_outputs_
        j[&#39;out_activation_&#39;]: self.estimator.out_activation_
    # elif USE_TORCH and isinstance(self._estimator, NonLinearEstimator):
    #     j[&#39;estimator_parameters&#39;] = self._estimator.to_json()
    
    return j</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pcgsepy.mapelites" href="index.html">pcgsepy.mapelites</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.get_emitter_by_str" href="#pcgsepy.mapelites.emitters.get_emitter_by_str">get_emitter_by_str</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter">ContextualBanditEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter.from_json" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter.pre_step" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter.reset" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.ContextualBanditEmitter.to_json" href="#pcgsepy.mapelites.emitters.ContextualBanditEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.Emitter" href="#pcgsepy.mapelites.emitters.Emitter">Emitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.Emitter.init_emitter" href="#pcgsepy.mapelites.emitters.Emitter.init_emitter">init_emitter</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.Emitter.pick_bin" href="#pcgsepy.mapelites.emitters.Emitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.Emitter.post_step" href="#pcgsepy.mapelites.emitters.Emitter.post_step">post_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.Emitter.pre_step" href="#pcgsepy.mapelites.emitters.Emitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.Emitter.reset" href="#pcgsepy.mapelites.emitters.Emitter.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter" href="#pcgsepy.mapelites.emitters.GreedyEmitter">GreedyEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter.from_json" href="#pcgsepy.mapelites.emitters.GreedyEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.GreedyEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter.pre_step" href="#pcgsepy.mapelites.emitters.GreedyEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter.reset" href="#pcgsepy.mapelites.emitters.GreedyEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.GreedyEmitter.to_json" href="#pcgsepy.mapelites.emitters.GreedyEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.HumanEmitter" href="#pcgsepy.mapelites.emitters.HumanEmitter">HumanEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.HumanEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.HumanEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanEmitter.reset" href="#pcgsepy.mapelites.emitters.HumanEmitter.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter">HumanPrefMatrixEmitter</a></code></h4>
<ul class="two-column">
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.from_json" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.init_emitter" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.init_emitter">init_emitter</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.post_step" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.post_step">post_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pre_step" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.reset" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.to_json" href="#pcgsepy.mapelites.emitters.HumanPrefMatrixEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.KNEmitter" href="#pcgsepy.mapelites.emitters.KNEmitter">KNEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.KNEmitter.from_json" href="#pcgsepy.mapelites.emitters.KNEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KNEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.KNEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KNEmitter.pre_step" href="#pcgsepy.mapelites.emitters.KNEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KNEmitter.reset" href="#pcgsepy.mapelites.emitters.KNEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KNEmitter.to_json" href="#pcgsepy.mapelites.emitters.KNEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.KernelEmitter" href="#pcgsepy.mapelites.emitters.KernelEmitter">KernelEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.KernelEmitter.from_json" href="#pcgsepy.mapelites.emitters.KernelEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KernelEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.KernelEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KernelEmitter.pre_step" href="#pcgsepy.mapelites.emitters.KernelEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KernelEmitter.reset" href="#pcgsepy.mapelites.emitters.KernelEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.KernelEmitter.to_json" href="#pcgsepy.mapelites.emitters.KernelEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.NonLinearEstimator" href="#pcgsepy.mapelites.emitters.NonLinearEstimator">NonLinearEstimator</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.NonLinearEstimator.train_estimator" href="#pcgsepy.mapelites.emitters.NonLinearEstimator.train_estimator">train_estimator</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitter" href="#pcgsepy.mapelites.emitters.OptimisingEmitter">OptimisingEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitter.from_json" href="#pcgsepy.mapelites.emitters.OptimisingEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.OptimisingEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitter.reset" href="#pcgsepy.mapelites.emitters.OptimisingEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitter.to_json" href="#pcgsepy.mapelites.emitters.OptimisingEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2">OptimisingEmitterV2</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2.from_json" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2.pick_bin" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2.reset" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.OptimisingEmitterV2.to_json" href="#pcgsepy.mapelites.emitters.OptimisingEmitterV2.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter">PreferenceBanditEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.from_json" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pre_step" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.reset" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.PreferenceBanditEmitter.to_json" href="#pcgsepy.mapelites.emitters.PreferenceBanditEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.RandomEmitter" href="#pcgsepy.mapelites.emitters.RandomEmitter">RandomEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.RandomEmitter.from_json" href="#pcgsepy.mapelites.emitters.RandomEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.RandomEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.RandomEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.RandomEmitter.reset" href="#pcgsepy.mapelites.emitters.RandomEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.RandomEmitter.to_json" href="#pcgsepy.mapelites.emitters.RandomEmitter.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter">SimpleTabularEmitter</a></code></h4>
<ul class="">
<li><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter.from_json" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter.from_json">from_json</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter.pick_bin" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter.pick_bin">pick_bin</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter.pre_step" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter.pre_step">pre_step</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter.reset" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter.reset">reset</a></code></li>
<li><code><a title="pcgsepy.mapelites.emitters.SimpleTabularEmitter.to_json" href="#pcgsepy.mapelites.emitters.SimpleTabularEmitter.to_json">to_json</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>